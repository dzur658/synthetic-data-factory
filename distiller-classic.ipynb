{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "08086bb9",
   "metadata": {},
   "source": [
    "# Distill Data âš—ï¸\n",
    "\n",
    "Distill data from an open source model!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61f29c6b",
   "metadata": {},
   "source": [
    "This is a basic synthetic data generator that uses SearXNG grounding, that does *not* require a model with tool calling capabilities. Feel free to adapt to your use case. ðŸ™‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5d2f2ed",
   "metadata": {},
   "source": [
    "## Imports\n",
    "These are the imports necessary to run the notebook, please also make sure you have Ollama along with the model you intend to use installed. If you don't have Ollama you can get it [here](https://ollama.com/) for Linux, MacOS, and Windows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "badff77d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dzurec/ai-projects/synthetic-data-factory/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import json\n",
    "\n",
    "import ollama\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bfee345",
   "metadata": {},
   "source": [
    "## Define Model and Inference Time Parameters\n",
    "Ensure you type the *exact* name of your Ollama model as it appears with `ollama list`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "beeb51e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"qwen3:1.7b-fp16\"\n",
    "# model = \"gemma3:270m-it-qat\"\n",
    "\n",
    "# Uncomment for different parameters as needed\n",
    "parameters = {\n",
    "    # Keeps the first N tokens of the prompt\n",
    "    # \"num_keep\": 5,\n",
    "    # For reproducibility, should probably be left commented out unless for experimental purposes\n",
    "    # \"seed\": 42,\n",
    "    # Number of tokens the model is allowed to generate\n",
    "    # \"num_predict\": 100,\n",
    "    # Narrow sampling to the top K tokens\n",
    "    # \"top_k\": 20,\n",
    "    # Add up probabilites for tokens after softmax until top_p percentage is reached; discard the rest\n",
    "    # \"top_p\": 0.9,\n",
    "    # Sampling method following the formula threshold = min_p * [LIKLIHOOD OF MOST PROBABLE TOKEN]\n",
    "    # Any token falling below the threshold is discarded\n",
    "    # \"min_p\": 0.0,\n",
    "    # Chooses \"human-like responses\" via entropy calculations; a value of 1.0 disables this sampling parameter\n",
    "    # \"typical_p\": 0.7,\n",
    "    # The window size for how far back in the context the model looks for the repeat penalty (the window)\n",
    "    # \"repeat_last_n\": 33,\n",
    "    # Sampling parameter that divides logits by the temperature, higher values result in more \"creativity\"; lower values result in less creativity\n",
    "    # and favor the most statistically likely output.\n",
    "    \"temperature\": 0.8,\n",
    "    # Modifies logit scores based on if the logit would be repeated in the window defined by `repeat_last_n`\n",
    "    # 1.0 disables this sampling parameter, less than 1.0 rewards repetiton, while greater than 1.0 penalizes repetition.\n",
    "    # \"repeat_penalty\": 1.2,\n",
    "    # Applies to the entire context, penalizes tokens that have already appeared in the context\n",
    "    # by subtracting the presence_penalty value from their logit score if they have already appeared.\n",
    "    # \"presence_penalty\": 1.5,\n",
    "    # Frequency penalty follows the following formula, being less aggressive than presence penalty,\n",
    "    # but still operating over the entire context:\n",
    "    # new_logit_score = original_logit_score - (frequency_penalty * number_of_times_token_has_appeared)\n",
    "    # \"frequency_penalty\": 1.0,\n",
    "    # Prevent the model from generating newline characters\n",
    "    # \"penalize_newline\": True,\n",
    "    # Add custom stop sequences to halt generation\n",
    "    # \"stop\": [\"\\n\", \"user:\"],\n",
    "    # Enable/disable numa for multi-socket systems\n",
    "    # \"numa\": False,\n",
    "    # The context window for the model\n",
    "    # \"num_ctx\": 1024,\n",
    "    # Reduces vram requirements for tokenization at the cost of speed with low values, inverse for higher values\n",
    "    # \"num_batch\": 2,\n",
    "    # Number of layers to offload to the GPU\n",
    "    # \"num_gpu\": 1,\n",
    "    # Define the main GPU if multiple are being used\n",
    "    # \"main_gpu\": 0,\n",
    "    # Most of the time should be kept True, uses memory-mapped files for model weights enabling\n",
    "    # lazy loading of weights into VRAM\n",
    "    # \"use_mmap\": True,\n",
    "    # Set to number of CPU cores for faster inference (Time To First Token)\n",
    "    \"num_thread\": 24\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ef9dfc2",
   "metadata": {},
   "source": [
    "## Test Ollama Connection\n",
    "Now let's test the model with the parameters and a simple question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "202e5f7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capital of France is **Paris**. It is the largest city in the country, known for its iconic landmarks such as the Eiffel Tower, the Louvre Museum, and the Seine River. Paris serves as the political, cultural, and economic center of France.\n"
     ]
    }
   ],
   "source": [
    "simple_question = \"What is the capital of France?\"\n",
    "\n",
    "response = ollama.chat(\n",
    "    model=model,\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": simple_question}\n",
    "        ],\n",
    "    options=parameters,\n",
    "    stream=False\n",
    ")\n",
    "\n",
    "print(response.message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d74965a2",
   "metadata": {},
   "source": [
    "## Special Tokens\n",
    "Next let's figure out the special tokens of the model. Make sure to change the `huggingface_model_name` to your model's name as it appears on the primary page for the model in the [Hub ðŸ¤—](https://huggingface.co). Ensure this is the normal safetensors version of the model and not the GGUF version. The code only downloads the tokenizer and not the full model file, and is only used in this step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8d5882a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eos_token': '<|im_end|>', 'pad_token': '<|endoftext|>', 'additional_special_tokens': ['<|im_start|>', '<|im_end|>', '<|object_ref_start|>', '<|object_ref_end|>', '<|box_start|>', '<|box_end|>', '<|quad_start|>', '<|quad_end|>', '<|vision_start|>', '<|vision_end|>', '<|vision_pad|>', '<|image_pad|>', '<|video_pad|>']}\n"
     ]
    }
   ],
   "source": [
    "huggingface_model_name = \"Qwen/Qwen3-1.7B\"\n",
    "# huggingface_model_name = \"google/gemma-3-270m-it\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(huggingface_model_name)\n",
    "special_tokens = tokenizer.special_tokens_map\n",
    "print(special_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fb0c1b1",
   "metadata": {},
   "source": [
    "## Question Generation Sequence\n",
    "We can use the following sequence of tokens to get the model to generate questions for us.\n",
    "\n",
    "`BEGINNING OF SEQUENCE TOKEN` + `HEADER START` + \"user\" + `END OF HEADER`\n",
    "\n",
    "This will cause the model to generate a question instead of a response like it typically would."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8c2a625f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>user\n",
      "The model should generate this!<|im_end|>\n",
      "<|im_start|>assistant\n",
      "\n",
      "<|im_start|>user\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_content = \"The model should generate this!\"\n",
    "\n",
    "# just define the user for the message with no content\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": test_content}\n",
    "]\n",
    "\n",
    "# apply the chat template\n",
    "prompt_sequence = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    # We want to get the strings associated with the special tokens\n",
    "    # not the ids so we set tokenize to false\n",
    "    tokenize=False,\n",
    "    # But we do want the generation prompt\n",
    "    add_generation_prompt=True\n",
    ")\n",
    "\n",
    "print(prompt_sequence)\n",
    "\n",
    "# split the prompt sequence into the beginning part we want\n",
    "position_of_search = prompt_sequence.find(test_content)\n",
    "\n",
    "question_generating_tokens = prompt_sequence[:position_of_search]\n",
    "\n",
    "print(question_generating_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef5e6476",
   "metadata": {},
   "source": [
    "## Get a Question\n",
    "Now let's try to get the model to generate a queston!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "58a2ef23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is the value of $x$ in the equation $ \\frac{1}{x} + \\frac{1}{x} + \\frac{1}{x} = \\frac{1}{x} $?\n"
     ]
    }
   ],
   "source": [
    "# Tell Ollama for this part we do not want the chat template to be applied\n",
    "parameters[\"raw\"] = True\n",
    "\n",
    "# This is the default if not set by the user\n",
    "previous_ctx = 1024\n",
    "\n",
    "# save ctx if changed by the user\n",
    "if \"num_ctx\" in parameters:\n",
    "    previous_ctx = parameters[\"num_ctx\"]\n",
    "\n",
    "# We want low context for question generation, the model will continue to address the question.\n",
    "parameters[\"num_ctx\"] = 256\n",
    "\n",
    "# Debugging if automatic formatting doesn't work for some reason\n",
    "# print(f\"Sequence: {question_generating_tokens}\")\n",
    "\n",
    "response = ollama.generate(\n",
    "    model=model,\n",
    "    prompt=question_generating_tokens,\n",
    "    options=parameters,\n",
    "    raw=True,\n",
    "    stream=False,\n",
    ")\n",
    "\n",
    "# Extract question\n",
    "question = response.response.split(\"\\n\")[0]\n",
    "\n",
    "# slighlty different response structure for ollama.generate\n",
    "print(question)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30398801",
   "metadata": {},
   "source": [
    "## Get an Answer\n",
    "We can now pass the question back to the model for the answer to it's question!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f26600fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question:\n",
      "What is the value of $x$ in the equation $ \\frac{1}{x} + \\frac{1}{x} + \\frac{1}{x} = \\frac{1}{x} $?\n",
      "--------------------\n",
      "Answer:\n",
      "To solve the equation:\n",
      "\n",
      "$$\n",
      "\\frac{3}{x} = \\frac{1}{x}\n",
      "$$\n",
      "\n",
      "we begin by eliminating the denominators. Since both sides have the same denominator $ x $, we can cross-multiply:\n",
      "\n",
      "$$\n",
      "3 \\cdot x = 1 \\cdot x\n",
      "$$\n",
      "\n",
      "Simplifying:\n",
      "\n",
      "$$\n",
      "3x = x\n",
      "$$\n",
      "\n",
      "Subtract $ x $ from both sides:\n",
      "\n",
      "$$\n",
      "3x - x = 0 \\quad \\Rightarrow \\quad 2x = 0\n",
      "$$\n",
      "\n",
      "Divide both sides by 2:\n",
      "\n",
      "$$\n",
      "x = 0\n",
      "$$\n",
      "\n",
      "---\n",
      "\n",
      "However, we must be cautious: the original equation contains $ \\frac{1}{x} $, which is undefined when $ x = 0 $. Therefore, $ x = 0 $ is not a valid solution because it makes the expression undefined.\n",
      "\n",
      "Now, consider the equation again:\n",
      "\n",
      "$$\n",
      "\\frac{3}{x} = \\frac{1}{x}\n",
      "$$\n",
      "\n",
      "We subtract $ \\frac{1}{x} $ from both sides:\n",
      "\n",
      "$$\n",
      "\\frac{3}{x} - \\frac{1}{x} = 0 \\quad \\Rightarrow \\quad \\frac{2}{x} = 0\n",
      "$$\n",
      "\n",
      "This implies that $ \\frac{2}{x} = 0 $, but a fraction can never equal zero unless the numerator is zero. Here, the numerator is 2, so this equation has no solution.\n",
      "\n",
      "---\n",
      "\n",
      "### Conclusion:\n",
      "\n",
      "The equation has **no solution** because it leads to a contradiction or an undefined expression.\n",
      "\n",
      "$$\n",
      "\\boxed{\\text{No solution}}\n",
      "$$\n"
     ]
    }
   ],
   "source": [
    "# we want to apply chat template again\n",
    "parameters[\"raw\"] = False\n",
    "\n",
    "# restore context length\n",
    "parameters[\"num_ctx\"] = previous_ctx\n",
    "\n",
    "# assume model supports thinking\n",
    "model_thinks = True\n",
    "\n",
    "try:\n",
    "    response = ollama.chat(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": question}\n",
    "            ],\n",
    "        options=parameters,\n",
    "        # try thinking\n",
    "        think=True,\n",
    "        stream=False,\n",
    "    )\n",
    "except ollama.ResponseError as e:\n",
    "    if \"does not support thinking\" in str(e):\n",
    "        print(\"Model does not support thinking, retrying without it.\")\n",
    "        response = ollama.chat(\n",
    "            model=model,\n",
    "            messages=[\n",
    "                {\"role\": \"user\", \"content\": question}\n",
    "                ],\n",
    "            options=parameters,\n",
    "            stream=False,\n",
    "        )\n",
    "\n",
    "        model_thinks = False\n",
    "\n",
    "print(\"Question:\")\n",
    "print(question)\n",
    "print(\"-\"*20)\n",
    "print(\"Answer:\")\n",
    "print(response.message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0827e847",
   "metadata": {},
   "source": [
    "## Thinking\n",
    "If desirable we can also extract the model's Chain of Thought as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "67802cd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Okay, so I need to solve the equation $\\frac{1}{x} + \\frac{1}{x} + \\frac{1}{x} = \\frac{1}{x}$. Let me think about how to approach this. \n",
      "\n",
      "First, let me rewrite the equation to make it clearer. The left side has three terms of $\\frac{1}{x}$ added together, and the right side is just $\\frac{1}{x}$. So, combining the left side, it's like saying $3 \\times \\frac{1}{x} = \\frac{1}{x}$. \n",
      "\n",
      "Hmm, so if I have three times something equals one times something else. Maybe I can subtract $\\frac{1}{x}$ from both sides to get the equation in a simpler form. Let me try that:\n",
      "\n",
      "Starting with:\n",
      "$$\n",
      "\\frac{1}{x} + \\frac{1}{x} + \\frac{1}{x} = \\frac{1}{x}\n",
      "$$\n",
      "Subtract $\\frac{1}{x}$ from both sides:\n",
      "$$\n",
      "\\frac{1}{x} + \\frac{1}{x} + \\frac{1}{x} - \\frac{1}{x} = \\frac{1}{x} - \\frac{1}{x}\n",
      "$$\n",
      "Simplifying both sides:\n",
      "Left side: $\\frac{1}{x} + \\frac{1}{x} + \\frac{1}{x} - \\frac{1}{x} = \\frac{1}{x} + \\frac{1}{x} = \\frac{2}{x}$\n",
      "Right side: $\\frac{1}{x} - \\frac{1}{x} = 0$\n",
      "\n",
      "So now the equation is $\\frac{2}{x} = 0$. \n",
      "\n",
      "Wait, but $\\frac{2}{x} = 0$... Hmm, is that possible? Let me think. If I have a fraction equal to zero, the numerator has to be zero, right? Because any number divided by something is zero only if the numerator is zero. So, $2 = 0$? But 2 isn't equal to 0. \n",
      "\n",
      "So, this suggests that there's no solution? But let me check my steps again to make sure I didn't make a mistake.\n",
      "\n",
      "Original equation: $3 \\times \\frac{1}{x} = \\frac{1}{x}$. \n",
      "\n",
      "Yes, that's correct. So subtracting $\\frac{1}{x}$ from both sides gives $2 \\times \\frac{1}{x} = 0$. Which implies that $\\frac{2}{x} = 0$. Which would require that $x$ is infinity? But infinity isn't a real number, so there's no real solution. \n",
      "\n",
      "But wait, maybe I need to consider if there are any restrictions on $x$. The original equation has $x$ in the denominator, so $x$ can't be zero. But even so, $\\frac{2}{x} = 0$ would require $x$ to be infinity, which is not allowed. Therefore, there is no solution. \n",
      "\n",
      "But let me try another approach. Maybe multiplying both sides by $x$ to eliminate denominators. Let's try that.\n",
      "\n",
      "Original equation:\n",
      "$$\n",
      "\\frac{1}{x} + \\frac{1}{x} + \\frac{1}{x} = \\frac{1}{x}\n",
      "$$\n",
      "Multiply both sides by $x$:\n",
      "Left side: $x \\times \\left( \\frac{1}{x} + \\frac{1}{x} + \\frac{1}{x} \\right) = x \\times \\frac{1}{x} + x \\times \\frac{1}{x} + x \\times \\frac{1}{x} = 1 + 1 + 1 = 3$\n",
      "Right side: $x \\times \\frac{1}{x} = 1$\n",
      "\n",
      "So now the equation becomes $3 = 1$, which is not true. \n",
      "\n",
      "Wait, that's another contradiction. So 3 equals 1? That can't be. So this suggests that there's no solution. \n",
      "\n",
      "So, both methods lead to a contradiction. Therefore, the equation has no solution. \n",
      "\n",
      "But let me check once more. Maybe there's a mistake in my arithmetic? Let me verify:\n",
      "\n",
      "Original equation: 3*(1/x) = 1/x. \n",
      "\n",
      "If I subtract 1/x from both sides: 2*(1/x) = 0. \n",
      "\n",
      "Yes, that's correct. So 2/x = 0. Which is impossible because 2/x is never zero for any real x. \n",
      "\n",
      "Alternatively, multiplying both sides by x gives 3 = 1, which is also impossible. \n",
      "\n",
      "Therefore, the conclusion is that there is no real solution. \n",
      "\n",
      "But sometimes, in some contexts, equations might be considered to have no solution, but in others, maybe there's a trick. Let me think again. \n",
      "\n",
      "Is there any value of x that can satisfy 3/x = 1/x? Let's solve 3/x = 1/x. \n",
      "\n",
      "Multiply both sides by x (assuming x â‰  0): 3 = 1. Which is impossible. \n",
      "\n",
      "Therefore, no solution. \n",
      "\n",
      "So the answer is that there is no solution. \n",
      "\n",
      "But the question is asking \"What is the solution?\" in the context of the problem. Maybe the answer is \"no solution\" or \"no real solution.\" \n",
      "\n",
      "But since the user might be expecting a box around the answer, and the problem is likely from a math context, maybe they expect to write that there is no solution. \n",
      "\n",
      "But let me check again. \n",
      "\n",
      "Original equation: 3/x = 1/x. \n",
      "\n",
      "Subtract 1/x: 2/x = 0. \n",
      "\n",
      "Which implies that 2 = 0, which is not possible. \n",
      "\n",
      "Therefore, no solution. \n",
      "\n",
      "Hence, the final answer is that there is no solution. \n",
      "\n",
      "But since the user might want the answer in a specific format, and in some cases, problems like this might have a trick, but in this case, it's straightforward. \n",
      "\n",
      "So, I think the answer is that there is no solution. \n",
      "\n",
      "**Final Answer**\n",
      "The equation has no solution, so the answer is \\boxed{\\text{No solution}}.\n",
      "\n",
      "Wait, but the instructions might expect a specific format. However, in many math problems, if there's no solution, it's possible to state that. But in the context of the original problem, maybe they expect a different answer. However, given the steps, there is no solution. So I think the answer is no solution.\n",
      "\n",
      "Alternatively, if the problem expects a numerical answer, but since there's no solution, perhaps the answer is \"no solution\" or \"none\". But in the required format, the user might expect a boxed answer. But since there's no solution, maybe they want to write that. However, in standard practice, if no solution exists, it's often denoted as \"no solution\" or \"no real solution\". However, in some cases, the system might expect a specific format. Since the user hasn't specified, but the initial instruction says to put the final answer within \\boxed{}, and in some cases, for no solution, they might use \\boxed{\\text{No solution}}. But I'm not sure if that's acceptable. Alternatively, maybe the problem expects a different approach. Let me check again.\n",
      "\n",
      "Wait, the original problem is: What is the solution? The equation is 3/x = 1/x. \n",
      "\n",
      "But if the problem is from a textbook or similar, maybe there's a mistake in the problem, but as per the given, the answer is no solution. \n",
      "\n",
      "Alternatively, maybe the problem is to find x, and there's a mistake in the problem. But as per the given information, no solution.\n",
      "\n",
      "So, following the instructions, I will put the final answer as \\boxed{\\text{No solution}}.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if model_thinks:\n",
    "    print(response.message.thinking)\n",
    "else:\n",
    "    print(\"Model does not support thinking.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc2b2b44",
   "metadata": {},
   "source": [
    "## Distill instruction/answer pairs\n",
    "Now we can put it all together and distill a CSV with as much data as we like from our model! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22d99dc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [04:46<00:00, 57.21s/it]\n"
     ]
    }
   ],
   "source": [
    "dataset_size = 5\n",
    "dataset_name = model + \"_distilled_dataset.json\"\n",
    "\n",
    "for i in tqdm(range(dataset_size)):\n",
    "    # Tell Ollama for this part we do not want the chat template to be applied\n",
    "    parameters[\"raw\"] = True\n",
    "\n",
    "    parameters[\"num_ctx\"] = 256\n",
    "\n",
    "    # Generate question\n",
    "    q_response = ollama.generate(\n",
    "        model=model,\n",
    "        prompt=question_generating_tokens,\n",
    "        options=parameters,\n",
    "        raw=True,\n",
    "        stream=False,\n",
    "    )\n",
    "\n",
    "    question = q_response.response.split(\"\\n\")[0]\n",
    "\n",
    "    parameters[\"num_ctx\"] = previous_ctx\n",
    "\n",
    "    # Generate answer\n",
    "    a_response = ollama.chat(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": question}\n",
    "            ],\n",
    "        options=parameters,\n",
    "        think=model_thinks,\n",
    "        stream=False,\n",
    "    )\n",
    "\n",
    "    entry = {\n",
    "        \"instruction\": question,\n",
    "        \"output\": a_response.message.content\n",
    "    }\n",
    "\n",
    "    if model_thinks:\n",
    "        entry[\"thinking\"] = a_response.message.thinking\n",
    "    \n",
    "    with open(dataset_name, \"a\") as f:\n",
    "        json.dump(entry, f, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "synthetic-data-factory",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
